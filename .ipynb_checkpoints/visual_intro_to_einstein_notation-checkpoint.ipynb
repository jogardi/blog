{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6d2eb84-b681-42f0-b689-a2cc2d98b21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19285ca7-7501-4986-a5bf-dd08c9651ddf",
   "metadata": {},
   "source": [
    "# A Visual Introduction to Einstein Notation and why you should Learn Tensor Calculus\n",
    "### Tensors are differential equations are polynomials\n",
    "\n",
    "Prerequisites:\n",
    "Install the math anywhere chrome plugin to see the equations rendered.\n",
    "Understand what a multidimensional array is. Hint: the \"tensors\" in PyTorch and TensorFlow are multidimensional arrays, not the tensors defined in tensor calculus.\n",
    "There is some python, but it is not essential.\n",
    "\n",
    "This is part of a series but can be read on its own.\n",
    "\n",
    "> Maxwell's equations… originally consisted of eight equations. These equations are not \"beautiful.\" They do not possess much symmetry. In their original form, they are ugly…. However, when rewritten using time as the fourth dimension, this rather awkward set of eight equations collapses into a single tensor equation. This is what a physicist calls \"beauty,\"- Michio Kaku\n",
    "\n",
    "When I started using Einstein notation for everything in 2020, it felt like a quirky thing to do. But since then, I've noticed the einsum function (short for einstein sums) becoming much more popular in matrix multiplications code. Einstein notation hasn't gained popularity in the latex of published papers, but still, the success of this einsum function is a step in the right direction, in my opinion. Please familiarize yourself with this function before continuing this post. I love the einsum function because it reminds us that so many deep learning methods with their fancy names are ultimately just long summations. However, the einsum function does not include the upper and lower indices used in the original einstein notation. The distinction between upper and lower indices is a generalization of duality. We call upper indices contravariant and lower indices covariant. For example, we could have a contravariant tensor aⁱ and a covariant tensor $b_i$. $a^i$ is a vector, while $b_i$ is a linear function that takes a vector such as $a^i$ as input and outputs a scalar. The notation $b_i a^i$ is equivalent to the functional nation $b_i(a^i)$. The output is defined as $b_i a^i=\\sum_i b_i a^i$. Note that $b$ and $a$ must have the same number of components. The distinction between upper and lower indices is useful for the following reasons.\n",
    "\n",
    "#### Reason 1: they should evoke different mental images\n",
    "Visualize a tensor with lower indices as a hyperplane, and visualize a tensor with upper indices should be visualized as a point at a particular location. The lower indices are dual to the upper indices, like orientation is dual to position.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81909dcc-4379-4772-b73e-1b15e237179f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"800\"\n",
       "            src=\"https://plotly.com/~jogardi/1.embed\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x10f794100>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://plotly.com/~jogardi/1.embed\", width=900, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3ab05e-3e66-48bf-9b65-cbd8e3ed4b3c",
   "metadata": {},
   "source": [
    "The two axes at the bottom of the figure above represent the inputs to the linear function $b_i$. The vertical axis represents the output. The green plane is the hyperplane to visualize when you see a tensor with one lower index like $b_i$. Alternatively, you can imagine $b_i$ as the purple arrow representing the direction of steepest descent along our hyperplane. If you were standing on this plane and then went in the direction of the purple arrow, you would be going directly uphill in the steepest direction. Note that while the direction of the arrow is accurate, its length is longer than it should be so that readers easily see it. The height of the orange point represents the output $b_ia^i$\n",
    "\n",
    "\n",
    "\n",
    "#### Reason 2: transposes are confusing\n",
    "Keeping track of the orientation of the matrix or vector often leads to trivial mistakes.\n",
    "\n",
    "#### Reason 3: invariance to coordinate system with transformation rules\n",
    "This post only explains the transformation rules for tensors with a single index.\n",
    "We can view many machine learning and physics concepts as a linear transformation that changes the coordinate system (i.e., change of basis). The name contravariant signifies that the change of basis transformation for contravariant (upper) indices is the inverse of the change of basis transformation for covariant (lower indices).\n",
    "When linear functions are applied to vectors so that the indices \"cancel out,\" the result is called a scalar. Scalars have the special property that they are invariant to change of basis because the scalar output will remain constant when we apply the transformation rules to the covector and contravariant tensor. Consider applying a change of coordinates $C(x)$ to $a_i$ and $b^i$. $C$ is a linear transformation, so it is associative and commutative. So when the transformation rules are applied , the output of $a_i b^i$ becomes $C(a_i)C^{−1}(b^i) = C(C^{−1}(a_i b^i)) = a_i b^i$. The transformation for contravariant indices is the inverse of the transformation for covariant indices so that the transformations cancel out when the linear function is applied to the vector. Therefore, the result is $a_i b^i$, which does not depend on the change of coordinates, $C$. So we say scalars are invariant to change of basis.\n",
    "\n",
    "#### Reason 4: Upper and lower indices help catch errors in our formulas\n",
    "If you know your formula should output a single number (a scalar), then each lower index (linear function) must pair with an upper index to take as input. The upper and lower indices should \"cancel out\" like units canceling out. So check your indices like you check your units.\n",
    "\n",
    "## More examples of how to visualize tensors\n",
    "* We can interpret a tensor with two upper indices as a point cloud\n",
    "* two upper indices for a \"square\" tensor where the length of each index is n can represent the pairwise relationships between a set of size $n$. For such a tensor $T$, $T^{ij}$ describes the similarity between the i'th and j'th elements. When the values of this tensor are zeros and ones, it can represent the adjacency matrix of a graph\n",
    "* We can use a tensor with 2 lower indices as a quadratic function. For a tensor, $B_{ij}$, the function $ B_{ij}a^ia^j$ is a quadratic function of a. See figure 1 for examples of the shapes a quadratic can make. Similarly, a tensor with 3 lower indices is a cubic.\n",
    "* 1 upper index and one lower index should be viewed as a linear transformation. See this video for how to visualize linear transformations https://www.khanacademy.org/math/linear-algebra/matrix-transformations/linear-transformations/a/visualizing-linear-transformations\n",
    "* a tensor with n lower indices corresponds to an n-degree polynomial. Consider a tensor $T_{ijk}$ with 3 lower indices applied to 3 vectors, $a$, $b$, $c$. The output would be $\\sum_i \\sum_j \\sum_k T_{i j k} a_i b_j c_k$. It is easier to imagine the case where $a = b = c$ and the vectors are 2-dimensional. You get the cubic curve $T_{0 0 0} a_0^3 + T_{0 0 1} a_0^2 a_1 + … + T_{1 1 1} a_1^3$.\n",
    "* A tensor $T^{ijk…}$ can be viewed as a function that takes natural numbers as inputs: $f(i, j, k, …) = T^{ijk…}$. Such a tensor could be used as a discrete basis function. Practical examples of tensors like this are:\n",
    "  - The stock price on each day could be represented by a tensor where $T^i$ is the stock price at the end of the i'th day.\n",
    "  - A black and white image can be viewed as a tensor T^{xy} where x and y are the coordinates of a pixel\n",
    "\n",
    "<figure>\n",
    "<img src=\"Topological-properties-of-the-quadratic-functions-in-two-variables-a-A-parabolic.png\" alt=\"Trulli\" style=\"width:100%\">\n",
    "    <figcaption align = \"center\"><center>Fig.1 - The tensors depicted in a, b, and c have different shapes, which we describe as positive definite, semi-positive definite, and indefinite respectively. The vertical dimension represents the quadratic output, and the bottom 2 dimensions represent the input vector where the input is a 2d vector.</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "## Unifying Polynomials, Differential Equations and All Kinds of Tensors\n",
    "\n",
    "There is a beautiful equation that brings together all kinds of tensors: Taylor's theorem generalized to higher dimensions. Given tensors containing the derivatives of a function $f(x)$ where $A_i$ contains the first derivatives, $B_{ij}$ contains the second derivatives, $C_{ijk}$ contains the third order derivatives and so on, the Taylor approximation to that function is: $\\frac{1}{1!} A_i x^i + \\frac{1}{2!}B_{ij} x^i x^j + \\frac{1}{3!}C_{ijk} x^i x^j x^k + …$ This can be further generalized to a function with multiple outputs as well as multiple inputs: $f(x)^u = \\frac{1}{1!} A^u_i x^i + \\frac{1}{2!}B^u_{ij} x^i x^j + \\frac{1}{3!}C^u_{ijk} x^i x^j x^k + ….$\n",
    "Here $A^u_i \\equiv \\frac{\\partial f(x)^u}{\\partial x^i}$. Similarly, $C^u_{ijk} \\equiv \\frac{\\partial f(x)^u}{\\partial x^i \\partial x^j \\partial x^k}$.\n",
    "Now maybe you have the intuition that there must be something deep and fundamental about this connection from tensors to polynomials. High schools put so much emphasis on polynomials but never explain why we should care. The tensor form of Taylor's theorem demonstrates how polynomials correspond to differential equations. Given equations that provide formulas for the derivatives of a function, we can use Taylor's theorem to obtain a polynomial that matches those derivatives. Since polynomials correspond to differential equations, we can view tensors as corresponding to derivatives. For example, given $f(a) = b_i a^i$, then\n",
    "Similarly, a tensor with two lower indices corresponds to a bunch of second-order derivatives: given $f(a) = B_{ij}a^i a^j$, then:\n",
    "If $B$ is symmetric, this simplifies to $2B_{ij}$.\n",
    "The original proof of Taylor's theorem involves infinities, but since we work with finite discrete data on computers, I instead provide an embarrassingly simple proof for the finite discrete case. We will show that an (n-1)-degree polynomial with coefficients $c^i$ can perfectly fit any n data points in the one-dimensional case where the data points are at uniform intervals.\n",
    "Let $y^i$ be the tensor containing the outputs where the index $i$ represents the input value. Then the Taylor series can be viewed as a change of basis where the coordinate transformation is $B^i_j = i^j$. Counterintuitively, the superscript in the $i^j$ part represents an exponent rather than an index notation 🙃. For each $j$, vector $B_j$ is like a discrete basis function as described earlier in this post. It is left as an exercise for the reader to show that these bases are linearly independent (see appendix A for the answer). That means that $B^i_j$ is like a matrix with linearly independent columns. Therefore the coordinate transformation $B^i_j$ is invertible. Then there exists some $c^j = (B^i_j)^{-1} y^i$ such that $y^i = B^i_j c^j = \\sum_j i^j c^j$ which is the (n-1)-degree polynomial that perfectly matches all n data points. Note that inverting a tensor turns upper indices to lower indices and vice versa.\n",
    "Now to make this more concrete let's implement polynomial regression in python. Given a dataset with inputs $X^i$ and corresponding outputs $y^i$, polynomial regression finds a polynomial that fits the data. Meaning we want coefficients $c_i$ such that $\\sum_j c_j (X^i)^j = y^i$.\n",
    "First, let's make some sample data and plot it. I use 4 data points here.\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "X = np.array(np.arange(0, 4), dtype=np.float32)\n",
    "y = np.random.randn(X.shape[0])\n",
    "plt.plot(X, y)\n",
    "```\n",
    "Then we define our coordinate transformation $B^i_j = (X^i)^j$:\n",
    "```python\n",
    "B = np.array([\n",
    "    [X[i]**j for j in range(len(X))]\n",
    "    for i in range(len(X))\n",
    "], dtype=np.float32)\n",
    "```\n",
    "See the plot below of our third basis vector, which is a cubic basis function:\n",
    "Then we use the inverse to get our coefficients $c_i$:\n",
    "```python\n",
    "c = np.einsum('ij,j->i', np.linalg.inv(B), y)\n",
    "```\n",
    "Finally, we evaluate $c_j B^i_j$ and check that it is very close to $y^j$:\n",
    "reconstructed_y = np.einsum('ij,j->i', B, c)\n",
    "print(np.max(np.abs(reconstructed_y - y)))\n",
    "That outputs 4.858050378642176e-08, which is extremely close to zero, so the reconstructed y from our polynomial is very close to the original y. It doesn't output exactly zero because there is some numerical error. If you play around with this code, you see the numerical instability issues worsen as the number of data points increases because of larger exponents in the higher degree polynomial. Arithmetic with huge exponents is numerically unstable if not done properly. So in practice, we usually break the data into chunks of three and fit a cubic polynomial to each piece.\n",
    "\n",
    "## See More Applications of Tensors\n",
    "I am writing a series that uses tensor notation ubiquitously. See my next post.\n",
    "\n",
    "---\n",
    "\n",
    "I hope you found this helpful. Any feedback or suggestions for this would be appreciated! 🙏\n",
    "\n",
    "## Appendix A: Showing that the polynomial basis vectors in B are linearly independent\n",
    "Suppose to derive a contradiction that they are linearly dependent. Then there exists some coefficients $c_k \\in \\mathbb{R^n}$ such that $c_k i^k = 0$ for all $i \\in 0,…,n-1$. That implies that $i^{n-1}=\\sum_{k=0}^{n-2} c_k i^k$. Taking the derivative of both sides with respect to i n-1 times yields $(n-1)!=0$ and that is a contradiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb47eb0-9f93-41e8-ab5c-5d5c96127558",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
